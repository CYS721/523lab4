{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "711502ab-d3c4-4a34-9e58-d2d9d27f0f27",
   "metadata": {},
   "source": [
    "# Interannotator Agreement Study Report\n",
    "\n",
    "Our annotators are four participants: Zhiyang Zhou, Huiyin Lam, Vivienne, and Yushun Cheng.\n",
    "\n",
    "## Annotation Process\n",
    "\n",
    "To ensure thorough coverage and consistency checks in our annotation process, we adopted a specific task distribution strategy, detailed as follows:\n",
    "\n",
    "- Zhiyang Zhou was responsible for annotating entries 1 to 300.\n",
    "- Huiyin Lam covered entries 201 to 500.\n",
    "- Vivienne handled entries 401 to 700.\n",
    "- Yushun Cheng took on entries 601 to 900.\n",
    "\n",
    "Furthermore, the last 100 entries (801-900) were annotated collectively by all four annotators, ensuring that each person had 100 entries overlapping with another annotator. This design allowed for 100 entries with a four-way overlap among us, aiming to enhance annotation consistency and permit a multifaceted evaluation of interannotator agreement.\n",
    "\n",
    "## Choice and Justification of Agreement Measure\n",
    "\n",
    "Given our annotation task involved multiple annotators classifying the same set of entries within a dataset, we selected **Fleiss' Kappa** as our measure of agreement to assess the overall consistency among multiple annotators in a categorical task. Additionally, for specific overlapping sections, we also calculated **Cohen's Kappa** to evaluate the pairwise consistency among annotators on particular entries. This comprehensive approach allowed us to assess agreement from both a general and detailed perspective.\n",
    "\n",
    "## Agreement Calculation and Results\n",
    "\n",
    "After completing the annotation conversion and preprocessing, we calculated the Fleiss' Kappa value for the entire dataset, which resulted in a approximate value of 0.9. This indicates a very high level of agreement among our annotators for the classification task. For specific overlapping areas, we further analyzed the agreement using Cohen's Kappa, which reinforced the high level of consistency among annotators, further validating the effectiveness of our annotation guidelines and training.\n",
    "\n",
    "## Discussion of Results\n",
    "\n",
    "Achieving a Fleiss' Kappa value of 0.9 signifies extremely high consistency in our annotation task, demonstrating the reliability of our annotation process. This outcome reflects the annotators' accurate understanding of the annotation guidelines and their consistency in performing the annotation task. Ensuring high consistency is crucial for any annotation project as it directly impacts the quality of data and the reliability of subsequent analyses.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The results of our agreement assessment are highly encouraging, not only showcasing the high degree of consistency among the annotation team but also proving the effectiveness of our annotation process and guidelines. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
